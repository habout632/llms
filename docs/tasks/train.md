

|GPT-2 (124M) | llm.c | 27 minutes | $9.2 | 8 x H100 |
https://x.com/Yuchenj_UW/status/1795850420503629994

|GPT-2 (124M) | llm.c | 43 minutes | $14.33 | 8 x A100 |
https://x.com/Yuchenj_UW/status/1795554739633221804

|GPT-2 (124M)| llm.c| 90 minutes| $20 | 8 x A100 |
https://github.com/karpathy/llm.c/discussions/481

Let's reproduce GPT-2 (1.6B): one 8XH100 node,| 24 hours | $672 | 8 x H100 
https://github.com/karpathy/llm.c/discussions/677



# Distributed Training Frameworks
multiple gpus, multiple nodes

## Huggingface Accelerate
https://github.com/huggingface/accelerate


## DeepSpeed
https://github.com/microsoft/DeepSpeed


## Pytorch Lightning
https://github.com/Lightning-AI/pytorch-lightning


## Horovod
https://github.com/horovod/horovod


